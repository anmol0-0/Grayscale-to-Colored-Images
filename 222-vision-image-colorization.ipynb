{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ed29c4",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa40247-10ab-49ad-a1e7-aa8908a061b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"openvino-dev>=2023.1.0\"\n",
    "%pip install gradio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae449c6-8a58-4d2c-8389-fde359ad7d1a",
   "metadata": {},
   "source": [
    "## Imports\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd439f78-a4b6-48b2-946e-a308383cf787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openvino as ov\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from IPython.core.display import HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6ca64-aa95-4e50-b5a6-975259d16dcc",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "* `PRECISION` - {FP16, FP32}, default: FP16.\n",
    "* `MODEL_DIR` - directory where the model is to be stored, default: public.\n",
    "* `MODEL_NAME` - name of the model used for inference, default: colorization-v2.\n",
    "* `DATA_DIR` - directory where test images are stored, default: data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc1b06-b3ae-40f2-8fca-1368455a369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECISION = \"FP16\"\n",
    "MODEL_DIR = \"models\"\n",
    "MODEL_NAME = \"colorization-v2\"\n",
    "MODEL_PATH = f\"{MODEL_DIR}/public/{MODEL_NAME}/{PRECISION}/{MODEL_NAME}.xml\"\n",
    "DATA_DIR = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab67524-0d85-462f-bfed-74851c53597b",
   "metadata": {},
   "source": [
    "### Select inference device\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf984617-8dbd-440e-b09d-8ac69809b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf87170-fd22-4fcf-b687-51b404be0db4",
   "metadata": {},
   "source": [
    "## Download the model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "`omz_downloader` downloads model files from online sources and, if necessary, patches them to make them more usable with Model Converter.\n",
    "\n",
    "In this case, `omz_downloader` downloads the checkpoint and pytorch model of [colorization-v2](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/public/colorization-v2/README.md) or [colorization-siggraph](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/colorization-siggraph) from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/public/index.md) and saves it under `MODEL_DIR`, as specified in the configuration above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679a06d8-d0ec-440f-ae7a-afc4ebe841e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_command = (\n",
    "    f\"omz_downloader \"\n",
    "    f\"--name {MODEL_NAME} \"\n",
    "    f\"--output_dir {MODEL_DIR} \"\n",
    "    f\"--cache_dir {MODEL_DIR}\"\n",
    ")\n",
    "! $download_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93715d09-a6b9-4af5-b129-462c0cb9fc27",
   "metadata": {},
   "source": [
    "## Convert the model to OpenVINO IR\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "`omz_converter` converts the models that are not in the OpenVINO™ IR format into that format using model conversion API.\n",
    "\n",
    "The downloaded pytorch model is not in OpenVINO IR format which is required for inference with OpenVINO runtime. `omz_converter` is used to convert the downloaded pytorch model into ONNX and OpenVINO IR format respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb56dd-0bd5-4a27-8588-91fb79a9b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    convert_command = (\n",
    "        f\"omz_converter \"\n",
    "        f\"--name {MODEL_NAME} \"\n",
    "        f\"--download_dir {MODEL_DIR} \"\n",
    "        f\"--precisions {PRECISION}\"\n",
    "    )\n",
    "    ! $convert_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8f0b29-dd61-469e-86e8-d7fb956a636c",
   "metadata": {},
   "source": [
    "## Loading the Model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Load the model in OpenVINO Runtime with `ie.read_model` and compile it for the specified device with `ie.compile_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc5285-1059-438b-80ef-7f2021b0e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "model = core.read_model(model=MODEL_PATH)\n",
    "compiled_model = core.compile_model(model=model, device_name=device.value)\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "N, C, H, W = list(input_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7681922b-5be1-4ec0-b48c-39af5e7bb44b",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47519c1-a6dd-468b-845d-96386033e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(impath: str) -> np.ndarray:\n",
    "    raw_image = cv2.imread(impath)\n",
    "    if raw_image.shape[2] > 1:\n",
    "        image = cv2.cvtColor(\n",
    "            cv2.cvtColor(raw_image, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2RGB\n",
    "        )\n",
    "    else:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "def plot_image(image: np.ndarray, title: str = \"\") -> None:\n",
    "    \n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_output(gray_img: np.ndarray, color_img: np.ndarray) -> None:\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    plt.title(\"Input\", fontsize=20)\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    plt.title(\"Colorized\", fontsize=20)\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    ax1.imshow(gray_img)\n",
    "    ax2.imshow(color_img)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d96b15-0176-45aa-9882-17e493acedff",
   "metadata": {},
   "source": [
    "## Load the Image\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1948813-77e6-434d-967a-5e0940cb40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_img_path_3 = \"C:\\\\Users\\\\anmol\\\\OneDrive\\\\Documents\\\\Korean Exchange 2024\\\\KimChi\\\\BackEnd\\\\222-vision-image-colorization\\\\data\\\\Editted.jpeg\"\n",
    "local_img_path_4 = \"C:\\\\Users\\\\anmol\\\\OneDrive\\\\Documents\\\\Korean Exchange 2024\\\\KimChi\\\\BackEnd\\\\222-vision-image-colorization\\\\data\\\\Editted 2.jpeg\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "shutil.copy(local_img_path_3, \"data/test_3.jpg\")\n",
    "shutil.copy(local_img_path_4, \"data/test_4.jpg\")\n",
    "\n",
    "\n",
    "\n",
    "test_img_3 = read_image(\"data/test_3.jpg\")\n",
    "test_img_4 = read_image(\"data/test_4.jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afccb409-06a0-4e94-8f8a-779628425a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize(gray_img: np.ndarray, model, output_layer, target_size=(W, H)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Colorize a grayscale image using a specified model.\n",
    "\n",
    "    Parameters:\n",
    "        gray_img (ndarray): Numpy array representing the original grayscale image.\n",
    "        model: The colorization model.\n",
    "        output_layer: The output layer of the colorization model.\n",
    "        target_size (tuple): Target size for resizing the image.\n",
    "\n",
    "    Returns:\n",
    "        colorized_image (ndarray): Numpy array depicting the colorized version of the original image.\n",
    "    \"\"\"\n",
    "    # Check if the input image is already in color format\n",
    "    if gray_img.shape[2] == 3:\n",
    "        img_rgb = gray_img.astype(np.float32) / 255\n",
    "    else:\n",
    "        img_rgb = cv2.cvtColor(gray_img.astype(np.float32) / 255, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Continue with the rest of the processing\n",
    "    img_lab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2Lab)\n",
    "    img_l_rs = cv2.resize(img_lab[:, :, 0], target_size)\n",
    "\n",
    "    # Inference\n",
    "    inputs = np.expand_dims(img_l_rs, axis=(0, 1))\n",
    "    res = model([inputs])[output_layer]\n",
    "    update_res = np.squeeze(res)\n",
    "\n",
    "    # Post-process\n",
    "    out = update_res.transpose((1, 2, 0))\n",
    "    out = cv2.resize(out, (gray_img.shape[1], gray_img.shape[0]))\n",
    "    img_lab_out = np.concatenate((img_lab[:, :, 0][:, :, np.newaxis], out), axis=2)\n",
    "    img_bgr_out = np.clip(cv2.cvtColor(img_lab_out, cv2.COLOR_Lab2RGB), 0, 1)\n",
    "    colorized_image = (cv2.resize(img_bgr_out, (gray_img.shape[1], gray_img.shape[0])) * 255).astype(np.uint8)\n",
    "\n",
    "    return colorized_image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2da5e-9059-4220-bb58-a1283fa7aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "color_img_3 = colorize(test_img_3, compiled_model, output_layer)\n",
    "color_img_4 = colorize(test_img_4, compiled_model, output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a950c4-7b94-4f84-82c7-3be7fddcc8e0",
   "metadata": {},
   "source": [
    "## Display Colorized Image\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e169e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_img_3, color_img_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_img_4, color_img_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2506d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "\n",
    "def colorize_image(input_image):\n",
    "    # Convert Gradio Image to numpy array\n",
    "    input_np = np.array(input_image)\n",
    "\n",
    "    # Perform colorization using your model (use the colorize function)\n",
    "    colorized_img = colorize(input_np, compiled_model, output_layer)\n",
    "\n",
    "    # Convert the result back to Gradio-compatible format (PIL Image)\n",
    "    result_pil = Image.fromarray(colorized_img)\n",
    "\n",
    "    return result_pil\n",
    "\n",
    "# Create Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=colorize_image,\n",
    "    inputs=gr.Image(),\n",
    "    outputs=gr.Image(),\n",
    "    live=True,\n",
    "    title=\"Gray-Scale to Colored[회색조에서 컬러로]\",\n",
    "    description=\"Click image icon, select image, AI converts to color, download result.[이미지 아이콘을 클릭하고 이미지를 선택하면 AI가 색상으로 변환하고 결과를 다운로드합니다.]\",\n",
    "    theme=\"gradio/monochrome\"\n",
    ")\n",
    "\n",
    "# Deploy the Gradio interface as a web app\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf53cb7",
   "metadata": {},
   "source": [
    "## Gradio Website \n",
    "To Use the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bed70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "\n",
    "def read_image(impath: str) -> np.ndarray:\n",
    "    raw_image = cv2.imread(impath)\n",
    "    return raw_image\n",
    "\n",
    "def convert_to_grayscale(color_img: np.ndarray) -> np.ndarray:\n",
    "    gray_img = cv2.cvtColor(color_img, cv2.COLOR_RGB2GRAY)\n",
    "    return gray_img\n",
    "\n",
    "def convert_image_to_grayscale(input_image):\n",
    "    # Convert Gradio Image to numpy array\n",
    "    input_np = np.array(input_image)\n",
    "\n",
    "    # Perform grayscale conversion using your function\n",
    "    grayscale_img = convert_to_grayscale(input_np)\n",
    "\n",
    "    # Convert the result back to Gradio-compatible format (PIL Image)\n",
    "    result_pil = Image.fromarray(grayscale_img)\n",
    "\n",
    "    return result_pil\n",
    "\n",
    "# Create Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=convert_image_to_grayscale,\n",
    "    inputs=gr.Image(),\n",
    "    outputs=gr.Image(),\n",
    "    live=True,\n",
    "    title=\"Colored to Grayscale Conversion\",\n",
    "    description=\"Click image icon, select colored image, AI converts to grayscale, download result.\",\n",
    "    theme=\"gradio/monochrome\"\n",
    ")\n",
    "\n",
    "# Deploy the Gradio interface as a web app\n",
    "iface.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
